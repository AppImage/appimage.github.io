{
  "name": "jan",
  "version": "0.4.5",
  "main": "./build/main.js",
  "author": "Jan <service@jan.ai>",
  "license": "MIT",
  "homepage": "https://github.com/janhq/jan/tree/main/electron",
  "description": "Use offline LLMs with your own data. Run open source models like Llama2 or Falcon on your internal computers/servers.",
  "dependencies": {
    "@alumna/reflect": "^1.1.3",
    "@janhq/core": "link:./core",
    "@janhq/server": "link:./server",
    "@npmcli/arborist": "^7.1.0",
    "@types/request": "^2.48.12",
    "@uiball/loaders": "^1.3.0",
    "electron-store": "^8.1.0",
    "electron-updater": "^6.1.7",
    "fs-extra": "^11.2.0",
    "node-fetch": "2",
    "pacote": "^17.0.4",
    "request": "^2.88.2",
    "request-progress": "^3.0.0",
    "rimraf": "^5.0.5",
    "typescript": "^5.3.3",
    "ulid": "^2.3.0",
    "use-debounce": "^9.0.4"
  },
  "installConfig": {
    "hoistingLimits": "workspaces"
  }
}